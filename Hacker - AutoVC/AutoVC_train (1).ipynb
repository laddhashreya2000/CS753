{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Copy of AutoVC.ipynb",
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "cdjoY0zye2Gt",
        "outputId": "2a83dbe8-33e0-4bfb-9796-667986dc48b9"
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!ls\n",
        "%cd '/content/drive/MyDrive/CS753'"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "klH68Uz6qDqU",
        "outputId": "c6623e39-fae2-4c80-a2e0-4ab902dba7f0"
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "drive  sample_data\n",
            "/content/drive/.shortcut-targets-by-id/1qWQwfDq56-I38shvF8oK-RW9bskBGg3h/CS753\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# !git clone https://github.com/auspicious3000/autovc\n",
        "# do not need to clone, but just download all models at same place and make it a working directory contaning wavs (data) folder"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "EuKVlJn5qzcm",
        "outputId": "83e0d1c9-a2ad-4690-ff51-b0fdfaa99a3b"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Cloning into 'autovc'...\n",
            "remote: Enumerating objects: 111, done.\u001b[K\n",
            "remote: Counting objects: 100% (7/7), done.\u001b[K\n",
            "remote: Compressing objects: 100% (7/7), done.\u001b[K\n",
            "remote: Total 111 (delta 2), reused 0 (delta 0), pack-reused 104\u001b[K\n",
            "Receiving objects: 100% (111/111), 8.18 MiB | 12.61 MiB/s, done.\n",
            "Resolving deltas: 100% (27/27), done.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "%cd autovc\n",
        "# working directory containing wavs folder"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "6vAvgeYbq2jx",
        "outputId": "76f18770-aa62-49ec-8ccb-751724c48cf8"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "/content/drive/.shortcut-targets-by-id/1qWQwfDq56-I38shvF8oK-RW9bskBGg3h/CS753/autovc\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# !ls"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "iq77zsg8q4wi",
        "outputId": "02f5bddd-0500-4203-ba1d-bb0cc5fc57cc"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "conversion.ipynb  main.py\t    model_bl.py  solver_encoder.py  wavs\n",
            "data_loader.py\t  make_metadata.py  model_vc.py  synthesis.py\n",
            "hparams.py\t  make_spect.py     README.md\t view\n",
            "LICENSE\t\t  metadata.pkl\t    results.pkl  vocoder.ipynb\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# !gdown --id 1SZPPnWAgpGrh0gQ7bXQJXXjOntbh4hmz\n",
        "#autovc ckpt"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "V5xZJdspq6IS",
        "outputId": "2c85a375-bb43-4acc-c705-f2bd340cc5c5"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/gdown/cli.py:131: FutureWarning: Option `--id` was deprecated in version 4.3.1 and will be removed in 5.0. You don't need to pass it anymore to use a file ID.\n",
            "  category=FutureWarning,\n",
            "Downloading...\n",
            "From: https://drive.google.com/uc?id=1SZPPnWAgpGrh0gQ7bXQJXXjOntbh4hmz\n",
            "To: /content/drive/MyDrive/CS753/autovc/autovc.ckpt\n",
            "100% 341M/341M [00:03<00:00, 98.4MB/s]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# !gdown --id 1Zksy0ndlDezo9wclQNZYkGi_6i7zi4nQ\n",
        "#wavenet vocoder"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "u7AA32LFrLWS",
        "outputId": "7283f616-ec10-4a52-e417-90c7603fca49"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/gdown/cli.py:131: FutureWarning: Option `--id` was deprecated in version 4.3.1 and will be removed in 5.0. You don't need to pass it anymore to use a file ID.\n",
            "  category=FutureWarning,\n",
            "Downloading...\n",
            "From: https://drive.google.com/uc?id=1Zksy0ndlDezo9wclQNZYkGi_6i7zi4nQ\n",
            "To: /content/drive/MyDrive/CS753/autovc/checkpoint_step001000000_ema.pth\n",
            "100% 297M/297M [00:03<00:00, 87.5MB/s]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# !gdown --id 1ORAeb4DlS_65WDkQN6LHx5dPyCM5PAVV\n",
        "#speaker encoder"
      ],
      "metadata": {
        "id": "2HmObuMAyVWu"
      },
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install wavenet_vocoder"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "BE3v5WPFui2h",
        "outputId": "28011d3f-816d-4323-a716-26cf821047fd"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: wavenet_vocoder in /usr/local/lib/python3.7/dist-packages (0.1.1)\n",
            "Requirement already satisfied: torch>=0.4.1 in /usr/local/lib/python3.7/dist-packages (from wavenet_vocoder) (1.11.0+cu113)\n",
            "Requirement already satisfied: scipy in /usr/local/lib/python3.7/dist-packages (from wavenet_vocoder) (1.4.1)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.7/dist-packages (from wavenet_vocoder) (1.21.6)\n",
            "Requirement already satisfied: typing-extensions in /usr/local/lib/python3.7/dist-packages (from torch>=0.4.1->wavenet_vocoder) (4.2.0)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "import numpy as np\n",
        "import os\n",
        "import pickle\n",
        "from math import ceil\n",
        "from tqdm import tqdm\n",
        "import librosa\n",
        "from wavenet_vocoder import builder\n",
        "import soundfile as sf\n",
        "from scipy import signal\n",
        "from scipy.signal import get_window\n",
        "from librosa.filters import mel\n",
        "from numpy.random import RandomState\n",
        "from torch.utils import data\n",
        "import time\n",
        "import datetime\n",
        "from collections import OrderedDict \n",
        "from multiprocessing import Process, Manager   \n",
        "import argparse\n",
        "from torch.backends import cudnn"
      ],
      "metadata": {
        "id": "sFi48Y0kF9ZZ"
      },
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#model_vc.py\n",
        "\n",
        "class LinearNorm(torch.nn.Module):\n",
        "    def __init__(self, in_dim, out_dim, bias=True, w_init_gain='linear'):\n",
        "        super(LinearNorm, self).__init__()\n",
        "        self.linear_layer = torch.nn.Linear(in_dim, out_dim, bias=bias)\n",
        "\n",
        "        torch.nn.init.xavier_uniform_(\n",
        "            self.linear_layer.weight,\n",
        "            gain=torch.nn.init.calculate_gain(w_init_gain))\n",
        "\n",
        "    def forward(self, x):\n",
        "        return self.linear_layer(x)\n",
        "\n",
        "\n",
        "class ConvNorm(torch.nn.Module):\n",
        "    def __init__(self, in_channels, out_channels, kernel_size=1, stride=1,\n",
        "                 padding=None, dilation=1, bias=True, w_init_gain='linear'):\n",
        "        super(ConvNorm, self).__init__()\n",
        "        if padding is None:\n",
        "            assert(kernel_size % 2 == 1)\n",
        "            padding = int(dilation * (kernel_size - 1) / 2)\n",
        "\n",
        "        self.conv = torch.nn.Conv1d(in_channels, out_channels,\n",
        "                                    kernel_size=kernel_size, stride=stride,\n",
        "                                    padding=padding, dilation=dilation,\n",
        "                                    bias=bias)\n",
        "\n",
        "        torch.nn.init.xavier_uniform_(\n",
        "            self.conv.weight, gain=torch.nn.init.calculate_gain(w_init_gain))\n",
        "\n",
        "    def forward(self, signal):\n",
        "        conv_signal = self.conv(signal)\n",
        "        return conv_signal\n",
        "\n",
        "\n",
        "class Encoder(nn.Module):\n",
        "    \"\"\"Encoder module:\n",
        "    \"\"\"\n",
        "    def __init__(self, dim_neck, dim_emb, freq):\n",
        "        super(Encoder, self).__init__()\n",
        "        self.dim_neck = dim_neck\n",
        "        self.freq = freq\n",
        "        \n",
        "        convolutions = []\n",
        "        for i in range(3):\n",
        "            conv_layer = nn.Sequential(\n",
        "                ConvNorm(80+dim_emb if i==0 else 512,\n",
        "                         512,\n",
        "                         kernel_size=5, stride=1,\n",
        "                         padding=2,\n",
        "                         dilation=1, w_init_gain='relu'),\n",
        "                nn.BatchNorm1d(512))\n",
        "            convolutions.append(conv_layer)\n",
        "        self.convolutions = nn.ModuleList(convolutions)\n",
        "        \n",
        "        self.lstm = nn.LSTM(512, dim_neck, 2, batch_first=True, bidirectional=True)\n",
        "\n",
        "    def forward(self, x, c_org):\n",
        "        x = x.squeeze(1).transpose(2,1)\n",
        "        c_org = c_org.unsqueeze(-1).expand(-1, -1, x.size(-1))\n",
        "        x = torch.cat((x, c_org), dim=1)\n",
        "        \n",
        "        for conv in self.convolutions:\n",
        "            x = F.relu(conv(x))\n",
        "        x = x.transpose(1, 2)\n",
        "        \n",
        "        self.lstm.flatten_parameters()\n",
        "        outputs, _ = self.lstm(x)\n",
        "        out_forward = outputs[:, :, :self.dim_neck]\n",
        "        out_backward = outputs[:, :, self.dim_neck:]\n",
        "        \n",
        "        codes = []\n",
        "        for i in range(0, outputs.size(1), self.freq):\n",
        "            codes.append(torch.cat((out_forward[:,i+self.freq-1,:],out_backward[:,i,:]), dim=-1))\n",
        "\n",
        "        return codes\n",
        "      \n",
        "        \n",
        "class Decoder(nn.Module):\n",
        "    \"\"\"Decoder module:\n",
        "    \"\"\"\n",
        "    def __init__(self, dim_neck, dim_emb, dim_pre):\n",
        "        super(Decoder, self).__init__()\n",
        "        \n",
        "        self.lstm1 = nn.LSTM(dim_neck*2+dim_emb, dim_pre, 1, batch_first=True)\n",
        "        \n",
        "        convolutions = []\n",
        "        for i in range(3):\n",
        "            conv_layer = nn.Sequential(\n",
        "                ConvNorm(dim_pre,\n",
        "                         dim_pre,\n",
        "                         kernel_size=5, stride=1,\n",
        "                         padding=2,\n",
        "                         dilation=1, w_init_gain='relu'),\n",
        "                nn.BatchNorm1d(dim_pre))\n",
        "            convolutions.append(conv_layer)\n",
        "        self.convolutions = nn.ModuleList(convolutions)\n",
        "        \n",
        "        self.lstm2 = nn.LSTM(dim_pre, 1024, 2, batch_first=True)\n",
        "        \n",
        "        self.linear_projection = LinearNorm(1024, 80)\n",
        "\n",
        "    def forward(self, x):\n",
        "        \n",
        "        #self.lstm1.flatten_parameters()\n",
        "        x, _ = self.lstm1(x)\n",
        "        x = x.transpose(1, 2)\n",
        "        \n",
        "        for conv in self.convolutions:\n",
        "            x = F.relu(conv(x))\n",
        "        x = x.transpose(1, 2)\n",
        "        \n",
        "        outputs, _ = self.lstm2(x)\n",
        "        \n",
        "        decoder_output = self.linear_projection(outputs)\n",
        "\n",
        "        return decoder_output   \n",
        "    \n",
        "    \n",
        "class Postnet(nn.Module):\n",
        "    \"\"\"Postnet\n",
        "        - Five 1-d convolution with 512 channels and kernel size 5\n",
        "    \"\"\"\n",
        "\n",
        "    def __init__(self):\n",
        "        super(Postnet, self).__init__()\n",
        "        self.convolutions = nn.ModuleList()\n",
        "\n",
        "        self.convolutions.append(\n",
        "            nn.Sequential(\n",
        "                ConvNorm(80, 512,\n",
        "                         kernel_size=5, stride=1,\n",
        "                         padding=2,\n",
        "                         dilation=1, w_init_gain='tanh'),\n",
        "                nn.BatchNorm1d(512))\n",
        "        )\n",
        "\n",
        "        for i in range(1, 5 - 1):\n",
        "            self.convolutions.append(\n",
        "                nn.Sequential(\n",
        "                    ConvNorm(512,\n",
        "                             512,\n",
        "                             kernel_size=5, stride=1,\n",
        "                             padding=2,\n",
        "                             dilation=1, w_init_gain='tanh'),\n",
        "                    nn.BatchNorm1d(512))\n",
        "            )\n",
        "\n",
        "        self.convolutions.append(\n",
        "            nn.Sequential(\n",
        "                ConvNorm(512, 80,\n",
        "                         kernel_size=5, stride=1,\n",
        "                         padding=2,\n",
        "                         dilation=1, w_init_gain='linear'),\n",
        "                nn.BatchNorm1d(80))\n",
        "            )\n",
        "\n",
        "    def forward(self, x):\n",
        "        for i in range(len(self.convolutions) - 1):\n",
        "            x = torch.tanh(self.convolutions[i](x))\n",
        "\n",
        "        x = self.convolutions[-1](x)\n",
        "\n",
        "        return x    \n",
        "    \n",
        "\n",
        "class Generator(nn.Module):\n",
        "    \"\"\"Generator network.\"\"\"\n",
        "    def __init__(self, dim_neck, dim_emb, dim_pre, freq):\n",
        "        super(Generator, self).__init__()\n",
        "        \n",
        "        self.encoder = Encoder(dim_neck, dim_emb, freq)\n",
        "        self.decoder = Decoder(dim_neck, dim_emb, dim_pre)\n",
        "        self.postnet = Postnet()\n",
        "\n",
        "    def forward(self, x, c_org, c_trg):\n",
        "                \n",
        "        codes = self.encoder(x, c_org)\n",
        "        if c_trg is None:\n",
        "            return torch.cat(codes, dim=-1)\n",
        "        \n",
        "        tmp = []\n",
        "        for code in codes:\n",
        "            tmp.append(code.unsqueeze(1).expand(-1,int(x.size(1)/len(codes)),-1))\n",
        "        code_exp = torch.cat(tmp, dim=1)\n",
        "        \n",
        "        encoder_outputs = torch.cat((code_exp, c_trg.unsqueeze(1).expand(-1,x.size(1),-1)), dim=-1)\n",
        "        \n",
        "        mel_outputs = self.decoder(encoder_outputs)\n",
        "                \n",
        "        mel_outputs_postnet = self.postnet(mel_outputs.transpose(2,1))\n",
        "        mel_outputs_postnet = mel_outputs + mel_outputs_postnet.transpose(2,1)\n",
        "        \n",
        "        mel_outputs = mel_outputs.unsqueeze(1)\n",
        "        mel_outputs_postnet = mel_outputs_postnet.unsqueeze(1)\n",
        "        \n",
        "        return mel_outputs, mel_outputs_postnet, torch.cat(codes, dim=-1)"
      ],
      "metadata": {
        "id": "Oxk637ixxDPa"
      },
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# # !python conversion.py\n",
        "\n",
        "# def pad_seq(x, base=32):\n",
        "#     len_out = int(base * ceil(float(x.shape[0])/base))\n",
        "#     len_pad = len_out - x.shape[0]\n",
        "#     assert len_pad >= 0\n",
        "#     return np.pad(x, ((0,len_pad),(0,0)), 'constant'), len_pad\n",
        "\n",
        "# device = 'cuda:0'\n",
        "# G = Generator(32,256,512,32).eval().to(device)\n",
        "\n",
        "# g_checkpoint = torch.load('autovc.ckpt', map_location='cpu')\n",
        "# G.load_state_dict(g_checkpoint['model'])\n",
        "\n",
        "# metadata = pickle.load(open('metadata.pkl', \"rb\"))\n",
        "\n",
        "# spect_vc = []\n",
        "\n",
        "# for sbmt_i in metadata:\n",
        "             \n",
        "#     x_org = sbmt_i[2]\n",
        "#     x_org, len_pad = pad_seq(x_org)\n",
        "#     uttr_org = torch.from_numpy(x_org[np.newaxis, :, :]).to(device)\n",
        "#     emb_org = torch.from_numpy(sbmt_i[1][np.newaxis, :]).to(device)\n",
        "    \n",
        "#     for sbmt_j in metadata:\n",
        "                   \n",
        "#         emb_trg = torch.from_numpy(sbmt_j[1][np.newaxis, :]).to(device)\n",
        "        \n",
        "#         with torch.no_grad():\n",
        "#             _, x_identic_psnt, _ = G(uttr_org, emb_org, emb_trg)\n",
        "            \n",
        "#         if len_pad == 0:\n",
        "#             uttr_trg = x_identic_psnt[0, 0, :, :].cpu().numpy()\n",
        "#         else:\n",
        "#             uttr_trg = x_identic_psnt[0, 0, :-len_pad, :].cpu().numpy()\n",
        "        \n",
        "#         spect_vc.append( ('{}x{}'.format(sbmt_i[0], sbmt_j[0]), uttr_trg) )\n",
        "        \n",
        "        \n",
        "# with open('results.pkl', 'wb') as handle:\n",
        "#     pickle.dump(spect_vc, handle)   "
      ],
      "metadata": {
        "id": "Pflo2-WOsb0A"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#hparams.py\n",
        "# NOTE: If you want full control for model architecture. please take a look\n",
        "# at the code and change whatever you want. Some hyper parameters are hardcoded.\n",
        "\n",
        "\n",
        "class Map(dict):\n",
        "\t\"\"\"\n",
        "    Example:\n",
        "    m = Map({'first_name': 'Eduardo'}, last_name='Pool', age=24, sports=['Soccer'])\n",
        "\n",
        "    Credits to epool:\n",
        "    https://stackoverflow.com/questions/2352181/how-to-use-a-dot-to-access-members-of-dictionary\n",
        "    \"\"\"\n",
        "\n",
        "\tdef __init__(self, *args, **kwargs):\n",
        "\t\tsuper(Map, self).__init__(*args, **kwargs)\n",
        "\t\tfor arg in args:\n",
        "\t\t\tif isinstance(arg, dict):\n",
        "\t\t\t\tfor k, v in arg.items():\n",
        "\t\t\t\t\tself[k] = v\n",
        "\n",
        "\t\tif kwargs:\n",
        "\t\t\tfor k, v in kwargs.iteritems():\n",
        "\t\t\t\tself[k] = v\n",
        "\n",
        "\tdef __getattr__(self, attr):\n",
        "\t\treturn self.get(attr)\n",
        "\n",
        "\tdef __setattr__(self, key, value):\n",
        "\t\tself.__setitem__(key, value)\n",
        "\n",
        "\tdef __setitem__(self, key, value):\n",
        "\t\tsuper(Map, self).__setitem__(key, value)\n",
        "\t\tself.__dict__.update({key: value})\n",
        "\n",
        "\tdef __delattr__(self, item):\n",
        "\t\tself.__delitem__(item)\n",
        "\n",
        "\tdef __delitem__(self, key):\n",
        "\t\tsuper(Map, self).__delitem__(key)\n",
        "\t\tdel self.__dict__[key]\n",
        "\n",
        "\n",
        "# Default hyperparameters:\n",
        "hparams = Map({\n",
        "\t'name': \"wavenet_vocoder\",\n",
        "\n",
        "\t# Convenient model builder\n",
        "\t'builder': \"wavenet\",\n",
        "\n",
        "\t# Input type:\n",
        "\t# 1. raw [-1, 1]\n",
        "\t# 2. mulaw [-1, 1]\n",
        "\t# 3. mulaw-quantize [0, mu]\n",
        "\t# If input_type is raw or mulaw, network assumes scalar input and\n",
        "\t# discretized mixture of logistic distributions output, otherwise one-hot\n",
        "\t# input and softmax output are assumed.\n",
        "\t# **NOTE**: if you change the one of the two parameters below, you need to\n",
        "\t# re-run preprocessing before training.\n",
        "\t'input_type': \"raw\",\n",
        "\t'quantize_channels': 65536,  # 65536 or 256\n",
        "\n",
        "\t# Audio:\n",
        "\t'sample_rate': 16000,\n",
        "\t# this is only valid for mulaw is True\n",
        "\t'silence_threshold': 2,\n",
        "\t'num_mels': 80,\n",
        "\t'fmin': 125,\n",
        "\t'fmax': 7600,\n",
        "\t'fft_size': 1024,\n",
        "\t# shift can be specified by either hop_size or frame_shift_ms\n",
        "\t'hop_size': 256,\n",
        "\t'frame_shift_ms': None,\n",
        "\t'min_level_db': -100,\n",
        "\t'ref_level_db': 20,\n",
        "\t# whether to rescale waveform or not.\n",
        "\t# Let x is an input waveform, rescaled waveform y is given by:\n",
        "\t# y = x / np.abs(x).max() * rescaling_max\n",
        "\t'rescaling': True,\n",
        "\t'rescaling_max': 0.999,\n",
        "\t# mel-spectrogram is normalized to [0, 1] for each utterance and clipping may\n",
        "\t# happen depends on min_level_db and ref_level_db, causing clipping noise.\n",
        "\t# If False, assertion is added to ensure no clipping happens.o0\n",
        "\t'allow_clipping_in_normalization': True,\n",
        "\n",
        "\t# Mixture of logistic distributions:\n",
        "\t'log_scale_min': float(-32.23619130191664),\n",
        "\n",
        "\t# Model:\n",
        "\t# This should equal to `quantize_channels` if mu-law quantize enabled\n",
        "\t# otherwise num_mixture * 3 (pi, mean, log_scale)\n",
        "\t'out_channels': 10 * 3,\n",
        "\t'layers': 24,\n",
        "\t'stacks': 4,\n",
        "\t'residual_channels': 512,\n",
        "\t'gate_channels': 512,  # split into 2 gropus internally for gated activation\n",
        "\t'skip_out_channels': 256,\n",
        "\t'dropout': 1 - 0.95,\n",
        "\t'kernel_size': 3,\n",
        "\t# If True, apply weight normalization as same as DeepVoice3\n",
        "\t'weight_normalization': True,\n",
        "\t# Use legacy code or not. Default is True since we already provided a model\n",
        "\t# based on the legacy code that can generate high-quality audio.\n",
        "\t# Ref: https://github.com/r9y9/wavenet_vocoder/pull/73\n",
        "\t'legacy': True,\n",
        "\n",
        "\t# Local conditioning (set negative value to disable))\n",
        "\t'cin_channels': 80,\n",
        "\t# If True, use transposed convolutions to upsample conditional features,\n",
        "\t# otherwise repeat features to adjust time resolution\n",
        "\t'upsample_conditional_features': True,\n",
        "\t# should np.prod(upsample_scales) == hop_size\n",
        "\t'upsample_scales': [4, 4, 4, 4],\n",
        "\t# Freq axis kernel size for upsampling network\n",
        "\t'freq_axis_kernel_size': 3,\n",
        "\n",
        "\t# Global conditioning (set negative value to disable)\n",
        "\t# currently limited for speaker embedding\n",
        "\t# this should only be enabled for multi-speaker dataset\n",
        "\t'gin_channels': -1,  # i.e., speaker embedding dim\n",
        "\t'n_speakers': -1,\n",
        "\n",
        "\t# Data loader\n",
        "\t'pin_memory': True,\n",
        "\t'num_workers': 2,\n",
        "\n",
        "\t# train/test\n",
        "\t# test size can be specified as portion or num samples\n",
        "\t'test_size': 0.0441,  # 50 for CMU ARCTIC single speaker\n",
        "\t'test_num_samples': None,\n",
        "\t'random_state': 1234,\n",
        "\n",
        "\t# Loss\n",
        "\n",
        "\t# Training:\n",
        "\t'batch_size': 2,\n",
        "\t'adam_beta1': 0.9,\n",
        "\t'adam_beta2': 0.999,\n",
        "\t'adam_eps': 1e-8,\n",
        "\t'amsgrad': False,\n",
        "\t'initial_learning_rate': 1e-3,\n",
        "\t# see lrschedule.py for available lr_schedule\n",
        "\t'lr_schedule': \"noam_learning_rate_decay\",\n",
        "\t'lr_schedule_kwargs': {},  # {\"anneal_rate\": 0.5, \"anneal_interval\": 50000},\n",
        "\t'nepochs': 2, #2000\n",
        "\t'weight_decay': 0.0,\n",
        "\t'clip_thresh': -1,\n",
        "\t# max time steps can either be specified as sec or steps\n",
        "\t# if both are None, then full audio samples are used in a batch\n",
        "\t'max_time_sec': None,\n",
        "\t'max_time_steps': 8000,\n",
        "\t# Hold moving averaged parameters and use them for evaluation\n",
        "\t'exponential_moving_average': True,\n",
        "\t# averaged = decay * averaged + (1 - decay) * x\n",
        "\t'ema_decay': 0.9999,\n",
        "\n",
        "\t# Save\n",
        "\t# per-step intervals\n",
        "\t'checkpoint_interval': 10, #1000\n",
        "\t'train_eval_interval': 1000, #1000\n",
        "\t# per-epoch interval\n",
        "\t'test_eval_epoch_interval': 5,\n",
        "\t'save_optimizer_state': True,\n",
        "\n",
        "\t# Eval:\n",
        "})\n",
        "\n",
        "\n",
        "def hparams_debug_string():\n",
        "\tvalues = hparams.values()\n",
        "\thp = ['  %s: %s' % (name, values[name]) for name in sorted(values)]\n",
        "\treturn 'Hyperparameters:\\n' + '\\n'.join(hp)"
      ],
      "metadata": {
        "id": "tR144lqo11YF"
      },
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#synthesis.py\n",
        "\"\"\"\n",
        "Synthesis waveform from trained WaveNet.\n",
        "\n",
        "Modified from https://github.com/r9y9/wavenet_vocoder\n",
        "\"\"\"\n",
        "torch.set_num_threads(4)\n",
        "use_cuda = torch.cuda.is_available()\n",
        "device = torch.device(\"cuda\" if use_cuda else \"cpu\")\n",
        "\n",
        "def build_model():\n",
        "    \n",
        "    model = getattr(builder, hparams.builder)(\n",
        "        out_channels=hparams.out_channels,\n",
        "        layers=hparams.layers,\n",
        "        stacks=hparams.stacks,\n",
        "        residual_channels=hparams.residual_channels,\n",
        "        gate_channels=hparams.gate_channels,\n",
        "        skip_out_channels=hparams.skip_out_channels,\n",
        "        cin_channels=hparams.cin_channels,\n",
        "        gin_channels=hparams.gin_channels,\n",
        "        weight_normalization=hparams.weight_normalization,\n",
        "        n_speakers=hparams.n_speakers,\n",
        "        dropout=hparams.dropout,\n",
        "        kernel_size=hparams.kernel_size,\n",
        "        upsample_conditional_features=hparams.upsample_conditional_features,\n",
        "        upsample_scales=hparams.upsample_scales,\n",
        "        freq_axis_kernel_size=hparams.freq_axis_kernel_size,\n",
        "        scalar_input=True,\n",
        "        legacy=hparams.legacy,\n",
        "    )\n",
        "    return model\n",
        "\n",
        "\n",
        "\n",
        "def wavegen(model, c=None, tqdm=tqdm):\n",
        "    \"\"\"Generate waveform samples by WaveNet.\n",
        "    \n",
        "    \"\"\"\n",
        "\n",
        "    model.eval()\n",
        "    model.make_generation_fast_()\n",
        "\n",
        "    Tc = c.shape[0]\n",
        "    upsample_factor = hparams.hop_size\n",
        "    # Overwrite length according to feature size\n",
        "    length = Tc * upsample_factor\n",
        "\n",
        "    # B x C x T\n",
        "    c = torch.FloatTensor(c.T).unsqueeze(0)\n",
        "\n",
        "    initial_input = torch.zeros(1, 1, 1).fill_(0.0)\n",
        "\n",
        "    # Transform data to GPU\n",
        "    initial_input = initial_input.to(device)\n",
        "    c = None if c is None else c.to(device)\n",
        "\n",
        "    with torch.no_grad():\n",
        "        y_hat = model.incremental_forward(\n",
        "            initial_input, c=c, g=None, T=length, tqdm=tqdm, softmax=True, quantize=True,\n",
        "            log_scale_min=hparams.log_scale_min)\n",
        "\n",
        "    y_hat = y_hat.view(-1).cpu().data.numpy()\n",
        "\n",
        "    return y_hat"
      ],
      "metadata": {
        "id": "s2q5B1WR1saW"
      },
      "execution_count": 31,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# ## vocoder.ipynb - Takes an hour to run\n",
        "\n",
        "# spect_vc = pickle.load(open('results.pkl', 'rb'))\n",
        "# device = torch.device(\"cuda\")\n",
        "# model = build_model().to(device)\n",
        "# checkpoint = torch.load(\"checkpoint_step001000000_ema.pth\")\n",
        "# model.load_state_dict(checkpoint[\"state_dict\"])\n",
        "\n",
        "# for spect in spect_vc:\n",
        "#     name = spect[0]\n",
        "#     c = spect[1]\n",
        "#     print(name)\n",
        "#     waveform = wavegen(model, c=c)   \n",
        "#     sf.write(name+'.wav', waveform, samplerate=16000)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "vPCIrqMCt4PP",
        "outputId": "32080310-6c76-4702-c34e-0441b3b5b729"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "p225xp225\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 23040/23040 [03:45<00:00, 102.27it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "p225xp228\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 23040/23040 [03:49<00:00, 100.18it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "p225xp256\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 23040/23040 [03:42<00:00, 103.69it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "p225xp270\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 23040/23040 [03:45<00:00, 102.33it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "p228xp225\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 22784/22784 [03:45<00:00, 101.08it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "p228xp228\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 22784/22784 [03:47<00:00, 100.00it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "p228xp256\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 22784/22784 [03:29<00:00, 108.53it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "p228xp270\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 22784/22784 [03:29<00:00, 108.52it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "p256xp225\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 19200/19200 [03:05<00:00, 103.40it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "p256xp228\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 19200/19200 [02:57<00:00, 108.34it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "p256xp256\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 19200/19200 [02:55<00:00, 109.37it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "p256xp270\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 19200/19200 [02:56<00:00, 108.90it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "p270xp225\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 27904/27904 [04:14<00:00, 109.52it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "p270xp228\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 27904/27904 [04:14<00:00, 109.67it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "p270xp256\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 27904/27904 [04:15<00:00, 109.24it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "p270xp270\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 27904/27904 [04:19<00:00, 107.55it/s]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Training"
      ],
      "metadata": {
        "id": "Dc3zyIDOvYHC"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# # Generate spectrogram from wav files - simple preprocessing using signal library\n",
        "# # !python make_spect.py\n",
        "\n",
        "# def butter_highpass(cutoff, fs, order=5):\n",
        "#     nyq = 0.5 * fs\n",
        "#     normal_cutoff = cutoff / nyq\n",
        "#     b, a = signal.butter(order, normal_cutoff, btype='high', analog=False)\n",
        "#     return b, a\n",
        "    \n",
        "    \n",
        "# def pySTFT(x, fft_length=1024, hop_length=256):\n",
        "    \n",
        "#     x = np.pad(x, int(fft_length//2), mode='reflect')\n",
        "    \n",
        "#     noverlap = fft_length - hop_length\n",
        "#     shape = x.shape[:-1]+((x.shape[-1]-noverlap)//hop_length, fft_length)\n",
        "#     strides = x.strides[:-1]+(hop_length*x.strides[-1], x.strides[-1])\n",
        "#     result = np.lib.stride_tricks.as_strided(x, shape=shape,\n",
        "#                                              strides=strides)\n",
        "    \n",
        "#     fft_window = get_window('hann', fft_length, fftbins=True)\n",
        "#     result = np.fft.rfft(fft_window * result, n=fft_length).T\n",
        "    \n",
        "#     return np.abs(result)    \n",
        "    \n",
        "    \n",
        "# mel_basis = mel(16000, 1024, fmin=90, fmax=7600, n_mels=80).T\n",
        "# min_level = np.exp(-100 / 20 * np.log(10))\n",
        "# b, a = butter_highpass(30, 16000, order=5)\n",
        "\n",
        "\n",
        "# # audio file directory\n",
        "# rootDir = './wavs'\n",
        "# # spectrogram directory\n",
        "# targetDir = './spmel'\n",
        "\n",
        "\n",
        "# dirName, subdirList, _ = next(os.walk(rootDir))\n",
        "# print('Found directory: %s' % dirName)\n",
        "\n",
        "# for subdir in sorted(subdirList):\n",
        "#     print(subdir)\n",
        "#     if not os.path.exists(os.path.join(targetDir, subdir)):\n",
        "#         os.makedirs(os.path.join(targetDir, subdir))\n",
        "#     _,_, fileList = next(os.walk(os.path.join(dirName,subdir)))\n",
        "#     prng = RandomState(int(subdir[1:])) \n",
        "#     for fileName in sorted(fileList):\n",
        "#         # Read audio file\n",
        "#         x, fs = sf.read(os.path.join(dirName,subdir,fileName))\n",
        "#         # Remove drifting noise\n",
        "#         y = signal.filtfilt(b, a, x)\n",
        "#         # Ddd a little random noise for model roubstness\n",
        "#         wav = y * 0.96 + (prng.rand(y.shape[0])-0.5)*1e-06\n",
        "#         # Compute spect\n",
        "#         D = pySTFT(wav).T\n",
        "#         # Convert to mel and normalize\n",
        "#         D_mel = np.dot(D, mel_basis)\n",
        "#         D_db = 20 * np.log10(np.maximum(min_level, D_mel)) - 16\n",
        "#         S = np.clip((D_db + 100) / 100, 0, 1)    \n",
        "#         # save spect    \n",
        "#         np.save(os.path.join(targetDir, subdir, fileName[:-4]),\n",
        "#                 S.astype(np.float32), allow_pickle=False)    "
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "we72oAciu0I4",
        "outputId": "1f3fd959-afc3-4526-cee2-de83e4058d88"
      },
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Found directory: ./wavs\n",
            "p225\n",
            "p226\n",
            "p227\n",
            "p228\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#model_bl\n",
        "\n",
        "class D_VECTOR(nn.Module):\n",
        "    \"\"\"d vector speaker embedding.\"\"\"\n",
        "    def __init__(self, num_layers=3, dim_input=40, dim_cell=256, dim_emb=64):\n",
        "        super(D_VECTOR, self).__init__()\n",
        "        self.lstm = nn.LSTM(input_size=dim_input, hidden_size=dim_cell, \n",
        "                            num_layers=num_layers, batch_first=True)  \n",
        "        self.embedding = nn.Linear(dim_cell, dim_emb)\n",
        "        \n",
        "    def forward(self, x):\n",
        "        self.lstm.flatten_parameters()            \n",
        "        lstm_out, _ = self.lstm(x)\n",
        "        embeds = self.embedding(lstm_out[:,-1,:])\n",
        "        norm = embeds.norm(p=2, dim=-1, keepdim=True) \n",
        "        embeds_normalized = embeds.div(norm)\n",
        "        return embeds_normalized"
      ],
      "metadata": {
        "id": "Rxd1Wvq_x_Ms"
      },
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Generate metadata including GE2E Speaker embedding\n",
        "# !python make_metadata.py\n",
        "\"\"\"\n",
        "Generate speaker embeddings and metadata for training\n",
        "\"\"\"\n",
        "\n",
        "C = D_VECTOR(dim_input=80, dim_cell=768, dim_emb=256).eval().cuda()\n",
        "c_checkpoint = torch.load('3000000-BL.ckpt')\n",
        "new_state_dict = OrderedDict()\n",
        "for key, val in c_checkpoint['model_b'].items():\n",
        "    new_key = key[7:]\n",
        "    new_state_dict[new_key] = val\n",
        "C.load_state_dict(new_state_dict)\n",
        "num_uttrs = 10\n",
        "len_crop = 128\n",
        "\n",
        "# Directory containing mel-spectrograms\n",
        "rootDir = './spmel'\n",
        "dirName, subdirList, _ = next(os.walk(rootDir))\n",
        "print('Found directory: %s' % dirName)\n",
        "\n",
        "\n",
        "speakers = []\n",
        "for speaker in sorted(subdirList):\n",
        "    print('Processing speaker: %s' % speaker)\n",
        "    utterances = []\n",
        "    utterances.append(speaker)\n",
        "    _, _, fileList = next(os.walk(os.path.join(dirName,speaker)))\n",
        "    \n",
        "    # make speaker embedding\n",
        "    assert len(fileList) >= num_uttrs\n",
        "    idx_uttrs = np.random.choice(len(fileList), size=num_uttrs, replace=False)\n",
        "    embs = []\n",
        "    for i in range(num_uttrs):\n",
        "        tmp = np.load(os.path.join(dirName, speaker, fileList[idx_uttrs[i]]))\n",
        "        candidates = np.delete(np.arange(len(fileList)), idx_uttrs)\n",
        "        # choose another utterance if the current one is too short\n",
        "        while tmp.shape[0] < len_crop:\n",
        "            idx_alt = np.random.choice(candidates)\n",
        "            tmp = np.load(os.path.join(dirName, speaker, fileList[idx_alt]))\n",
        "            candidates = np.delete(candidates, np.argwhere(candidates==idx_alt))\n",
        "        left = np.random.randint(0, tmp.shape[0]-len_crop)\n",
        "        melsp = torch.from_numpy(tmp[np.newaxis, left:left+len_crop, :]).cuda()\n",
        "        emb = C(melsp)\n",
        "        embs.append(emb.detach().squeeze().cpu().numpy())     \n",
        "    utterances.append(np.mean(embs, axis=0))\n",
        "    \n",
        "    # create file list\n",
        "    for fileName in sorted(fileList):\n",
        "        utterances.append(os.path.join(speaker,fileName))\n",
        "    speakers.append(utterances)\n",
        "    \n",
        "with open(os.path.join(rootDir, 'train.pkl'), 'wb') as handle:\n",
        "    pickle.dump(speakers, handle)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "dFA56TvEvfLZ",
        "outputId": "d91bb847-9f32-41de-f4ac-3a64baab4a87"
      },
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Found directory: ./spmel\n",
            "Processing speaker: p225\n",
            "Processing speaker: p226\n",
            "Processing speaker: p227\n",
            "Processing speaker: p228\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#solver_encoder.py\n",
        "\n",
        "class Solver(object):\n",
        "\n",
        "    def __init__(self, vcc_loader, config):\n",
        "        \"\"\"Initialize configurations.\"\"\"\n",
        "\n",
        "        # Data loader.\n",
        "        self.vcc_loader = vcc_loader\n",
        "\n",
        "        # Model configurations.\n",
        "        self.lambda_cd = config.lambda_cd\n",
        "        self.dim_neck = config.dim_neck\n",
        "        self.dim_emb = config.dim_emb\n",
        "        self.dim_pre = config.dim_pre\n",
        "        self.freq = config.freq\n",
        "\n",
        "        # Training configurations.\n",
        "        self.batch_size = config.batch_size\n",
        "        self.num_iters = config.num_iters\n",
        "        \n",
        "        # Miscellaneous.\n",
        "        self.use_cuda = torch.cuda.is_available()\n",
        "        self.device = torch.device('cuda:0' if self.use_cuda else 'cpu')\n",
        "        self.log_step = config.log_step\n",
        "\n",
        "        # Build the model and tensorboard.\n",
        "        self.build_model()\n",
        "\n",
        "            \n",
        "    def build_model(self):\n",
        "        \n",
        "        self.G = Generator(self.dim_neck, self.dim_emb, self.dim_pre, self.freq)        \n",
        "        \n",
        "        self.g_optimizer = torch.optim.Adam(self.G.parameters(), 0.0001)\n",
        "        \n",
        "        self.G.to(self.device)\n",
        "        \n",
        "\n",
        "    def reset_grad(self):\n",
        "        \"\"\"Reset the gradient buffers.\"\"\"\n",
        "        self.g_optimizer.zero_grad()\n",
        "      \n",
        "    \n",
        "    #=====================================================================================================================================#\n",
        "    \n",
        "    \n",
        "                \n",
        "    def train(self):\n",
        "        # Set data loader.\n",
        "        data_loader = self.vcc_loader\n",
        "        \n",
        "        # Print logs in specified order\n",
        "        keys = ['G/loss_id','G/loss_id_psnt','G/loss_cd']\n",
        "            \n",
        "        # Start training.\n",
        "        print('Start training...')\n",
        "        start_time = time.time()\n",
        "        for i in range(self.num_iters):\n",
        "\n",
        "            # =================================================================================== #\n",
        "            #                             1. Preprocess input data                                #\n",
        "            # =================================================================================== #\n",
        "\n",
        "            # Fetch data.\n",
        "            try:\n",
        "                x_real, emb_org = next(data_iter)\n",
        "            except:\n",
        "                data_iter = iter(data_loader)\n",
        "                x_real, emb_org = next(data_iter)\n",
        "            \n",
        "            \n",
        "            x_real = x_real.to(self.device) \n",
        "            emb_org = emb_org.to(self.device) \n",
        "                        \n",
        "       \n",
        "            # =================================================================================== #\n",
        "            #                               2. Train the generator                                #\n",
        "            # =================================================================================== #\n",
        "            \n",
        "            self.G = self.G.train()\n",
        "            torch.save(self.G.state_dict(), \"./model.pt\")            \n",
        "            # Identity mapping loss\n",
        "            x_identic, x_identic_psnt, code_real = self.G(x_real, emb_org, emb_org)\n",
        "            g_loss_id = F.mse_loss(x_real, x_identic)   \n",
        "            g_loss_id_psnt = F.mse_loss(x_real, x_identic_psnt)   \n",
        "            \n",
        "            # Code semantic loss.\n",
        "            code_reconst = self.G(x_identic_psnt, emb_org, None)\n",
        "            g_loss_cd = F.l1_loss(code_real, code_reconst)\n",
        "\n",
        "\n",
        "            # Backward and optimize.\n",
        "            g_loss = g_loss_id + g_loss_id_psnt + self.lambda_cd * g_loss_cd\n",
        "            self.reset_grad()\n",
        "            g_loss.backward()\n",
        "            self.g_optimizer.step()\n",
        "\n",
        "            # Logging.\n",
        "            loss = {}\n",
        "            loss['G/loss_id'] = g_loss_id.item()\n",
        "            loss['G/loss_id_psnt'] = g_loss_id_psnt.item()\n",
        "            loss['G/loss_cd'] = g_loss_cd.item()\n",
        "\n",
        "            # =================================================================================== #\n",
        "            #                                 4. Miscellaneous                                    #\n",
        "            # =================================================================================== #\n",
        "\n",
        "            # Print out training information.\n",
        "            if (i+1) % self.log_step == 0:\n",
        "                et = time.time() - start_time\n",
        "                et = str(datetime.timedelta(seconds=et))[:-7]\n",
        "                log = \"Elapsed [{}], Iteration [{}/{}]\".format(et, i+1, self.num_iters)\n",
        "                for tag in keys:\n",
        "                    log += \", {}: {:.4f}\".format(tag, loss[tag])\n",
        "                print(log)"
      ],
      "metadata": {
        "id": "az5MkXmBzKkK"
      },
      "execution_count": 25,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#data_loader.py\n",
        "\n",
        "class Utterances(data.Dataset):\n",
        "    \"\"\"Dataset class for the Utterances dataset.\"\"\"\n",
        "\n",
        "    def __init__(self, root_dir, len_crop):\n",
        "        \"\"\"Initialize and preprocess the Utterances dataset.\"\"\"\n",
        "        self.root_dir = root_dir\n",
        "        self.len_crop = len_crop\n",
        "        self.step = 10\n",
        "        \n",
        "        metaname = os.path.join(self.root_dir, \"train.pkl\")\n",
        "        meta = pickle.load(open(metaname, \"rb\"))\n",
        "        \n",
        "        \"\"\"Load data using multiprocessing\"\"\"\n",
        "        manager = Manager()\n",
        "        meta = manager.list(meta)\n",
        "        dataset = manager.list(len(meta)*[None])  \n",
        "        processes = []\n",
        "        for i in range(0, len(meta), self.step):\n",
        "            p = Process(target=self.load_data, \n",
        "                        args=(meta[i:i+self.step],dataset,i))  \n",
        "            p.start()\n",
        "            processes.append(p)\n",
        "        for p in processes:\n",
        "            p.join()\n",
        "            \n",
        "        self.train_dataset = list(dataset)\n",
        "        self.num_tokens = len(self.train_dataset)\n",
        "        \n",
        "        print('Finished loading the dataset...')\n",
        "        \n",
        "        \n",
        "    def load_data(self, submeta, dataset, idx_offset):  \n",
        "        for k, sbmt in enumerate(submeta):    \n",
        "            uttrs = len(sbmt)*[None]\n",
        "            for j, tmp in enumerate(sbmt):\n",
        "                if j < 2:  # fill in speaker id and embedding\n",
        "                    uttrs[j] = tmp\n",
        "                else: # load the mel-spectrograms\n",
        "                    uttrs[j] = np.load(os.path.join(self.root_dir, tmp))\n",
        "            dataset[idx_offset+k] = uttrs\n",
        "                   \n",
        "        \n",
        "    def __getitem__(self, index):\n",
        "        # pick a random speaker\n",
        "        dataset = self.train_dataset \n",
        "        list_uttrs = dataset[index]\n",
        "        emb_org = list_uttrs[1]\n",
        "        \n",
        "        # pick random uttr with random crop\n",
        "        a = np.random.randint(2, len(list_uttrs))\n",
        "        tmp = list_uttrs[a]\n",
        "        if tmp.shape[0] < self.len_crop:\n",
        "            len_pad = self.len_crop - tmp.shape[0]\n",
        "            uttr = np.pad(tmp, ((0,len_pad),(0,0)), 'constant')\n",
        "        elif tmp.shape[0] > self.len_crop:\n",
        "            left = np.random.randint(tmp.shape[0]-self.len_crop)\n",
        "            uttr = tmp[left:left+self.len_crop, :]\n",
        "        else:\n",
        "            uttr = tmp\n",
        "        \n",
        "        return uttr, emb_org\n",
        "    \n",
        "\n",
        "    def __len__(self):\n",
        "        \"\"\"Return the number of spkrs.\"\"\"\n",
        "        return self.num_tokens\n",
        "\n",
        "def get_loader(root_dir, batch_size=16, len_crop=128, num_workers=0):\n",
        "    \"\"\"Build and return a data loader.\"\"\"\n",
        "    \n",
        "    dataset = Utterances(root_dir, len_crop)\n",
        "    \n",
        "    worker_init_fn = lambda x: np.random.seed((torch.initial_seed()) % (2**32))\n",
        "    data_loader = data.DataLoader(dataset=dataset,\n",
        "                                  batch_size=batch_size,\n",
        "                                  shuffle=True,\n",
        "                                  num_workers=num_workers,\n",
        "                                  drop_last=True,\n",
        "                                  worker_init_fn=worker_init_fn)\n",
        "    return data_loader"
      ],
      "metadata": {
        "id": "XctSNuEFzUJj"
      },
      "execution_count": 26,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# !python main.py\n",
        "\n",
        "def str2bool(v):\n",
        "    return v.lower() in ('true')\n",
        "\n",
        "# def main(config   \n",
        "# if __name__ == '__main__':\n",
        "\n",
        "parser = argparse.ArgumentParser()\n",
        "\n",
        "# Model configuration.\n",
        "parser.add_argument('--lambda_cd', type=float, default=1, help='weight for hidden code loss')\n",
        "parser.add_argument('--dim_neck', type=int, default=16)\n",
        "parser.add_argument('--dim_emb', type=int, default=256)\n",
        "parser.add_argument('--dim_pre', type=int, default=512)\n",
        "parser.add_argument('--freq', type=int, default=16)\n",
        "\n",
        "# Training configuration.\n",
        "parser.add_argument('--data_dir', type=str, default='./spmel')\n",
        "parser.add_argument('--batch_size', type=int, default=2, help='mini-batch size')\n",
        "parser.add_argument('--num_iters', type=int, default=10000, help='number of total iterations')\n",
        "parser.add_argument('--len_crop', type=int, default=128, help='dataloader output sequence length')\n",
        "\n",
        "# Miscellaneous.\n",
        "parser.add_argument('--log_step', type=int, default=10)\n",
        "\n",
        "config = parser.parse_args(args=[])\n",
        "print(config)\n",
        "# main(config)\n",
        "\n",
        "# For fast training.\n",
        "cudnn.benchmark = True\n",
        "\n",
        "# Data loader.\n",
        "vcc_loader = get_loader(config.data_dir, config.batch_size, config.len_crop)\n",
        "\n",
        "solver = Solver(vcc_loader, config)\n",
        "\n",
        "solver.train()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "L1qfHtf4vkDC",
        "outputId": "5985da79-e34c-4175-f390-7bca93c43281"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Namespace(batch_size=2, data_dir='./spmel', dim_emb=256, dim_neck=16, dim_pre=512, freq=16, lambda_cd=1, len_crop=128, log_step=10, num_iters=10000)\n",
            "Finished loading the dataset...\n",
            "Start training...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/ipykernel_launcher.py:85: UserWarning: Using a target size (torch.Size([2, 1, 128, 80])) that is different to the input size (torch.Size([2, 128, 80])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
            "/usr/local/lib/python3.7/dist-packages/ipykernel_launcher.py:86: UserWarning: Using a target size (torch.Size([2, 1, 128, 80])) that is different to the input size (torch.Size([2, 128, 80])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Elapsed [0:00:06], Iteration [10/10000], G/loss_id: 0.1405, G/loss_id_psnt: 1.1247, G/loss_cd: 0.1284\n",
            "Elapsed [0:00:14], Iteration [20/10000], G/loss_id: 0.0622, G/loss_id_psnt: 0.9753, G/loss_cd: 0.1038\n",
            "Elapsed [0:00:21], Iteration [30/10000], G/loss_id: 0.0613, G/loss_id_psnt: 0.8598, G/loss_cd: 0.1010\n",
            "Elapsed [0:00:28], Iteration [40/10000], G/loss_id: 0.1165, G/loss_id_psnt: 0.6140, G/loss_cd: 0.1064\n",
            "Elapsed [0:00:35], Iteration [50/10000], G/loss_id: 0.1887, G/loss_id_psnt: 0.4422, G/loss_cd: 0.0982\n",
            "Elapsed [0:00:42], Iteration [60/10000], G/loss_id: 0.2790, G/loss_id_psnt: 0.3101, G/loss_cd: 0.0918\n",
            "Elapsed [0:00:50], Iteration [70/10000], G/loss_id: 0.2528, G/loss_id_psnt: 0.3211, G/loss_cd: 0.0819\n",
            "Elapsed [0:00:58], Iteration [80/10000], G/loss_id: 0.2796, G/loss_id_psnt: 0.2697, G/loss_cd: 0.0662\n",
            "Elapsed [0:01:05], Iteration [90/10000], G/loss_id: 0.2852, G/loss_id_psnt: 0.2766, G/loss_cd: 0.0718\n",
            "Elapsed [0:01:12], Iteration [100/10000], G/loss_id: 0.2461, G/loss_id_psnt: 0.3171, G/loss_cd: 0.0641\n",
            "Elapsed [0:01:19], Iteration [110/10000], G/loss_id: 0.2574, G/loss_id_psnt: 0.2861, G/loss_cd: 0.0591\n",
            "Elapsed [0:01:26], Iteration [120/10000], G/loss_id: 0.3069, G/loss_id_psnt: 0.2418, G/loss_cd: 0.0572\n",
            "Elapsed [0:01:34], Iteration [130/10000], G/loss_id: 0.3099, G/loss_id_psnt: 0.2323, G/loss_cd: 0.0483\n",
            "Elapsed [0:01:41], Iteration [140/10000], G/loss_id: 0.2291, G/loss_id_psnt: 0.3286, G/loss_cd: 0.0594\n",
            "Elapsed [0:01:48], Iteration [150/10000], G/loss_id: 0.2244, G/loss_id_psnt: 0.3379, G/loss_cd: 0.0526\n",
            "Elapsed [0:01:56], Iteration [160/10000], G/loss_id: 0.2748, G/loss_id_psnt: 0.2552, G/loss_cd: 0.0343\n",
            "Elapsed [0:02:03], Iteration [170/10000], G/loss_id: 0.2647, G/loss_id_psnt: 0.2677, G/loss_cd: 0.0450\n",
            "Elapsed [0:02:11], Iteration [180/10000], G/loss_id: 0.2580, G/loss_id_psnt: 0.2750, G/loss_cd: 0.0380\n",
            "Elapsed [0:02:19], Iteration [190/10000], G/loss_id: 0.2835, G/loss_id_psnt: 0.2530, G/loss_cd: 0.0339\n",
            "Elapsed [0:02:27], Iteration [200/10000], G/loss_id: 0.2761, G/loss_id_psnt: 0.2557, G/loss_cd: 0.0375\n",
            "Elapsed [0:02:34], Iteration [210/10000], G/loss_id: 0.2695, G/loss_id_psnt: 0.2680, G/loss_cd: 0.0341\n",
            "Elapsed [0:02:41], Iteration [220/10000], G/loss_id: 0.2563, G/loss_id_psnt: 0.2686, G/loss_cd: 0.0386\n",
            "Elapsed [0:02:49], Iteration [230/10000], G/loss_id: 0.2653, G/loss_id_psnt: 0.2625, G/loss_cd: 0.0313\n",
            "Elapsed [0:02:56], Iteration [240/10000], G/loss_id: 0.2582, G/loss_id_psnt: 0.2646, G/loss_cd: 0.0297\n",
            "Elapsed [0:03:04], Iteration [250/10000], G/loss_id: 0.2727, G/loss_id_psnt: 0.2537, G/loss_cd: 0.0303\n",
            "Elapsed [0:03:12], Iteration [260/10000], G/loss_id: 0.2624, G/loss_id_psnt: 0.2677, G/loss_cd: 0.0220\n",
            "Elapsed [0:03:19], Iteration [270/10000], G/loss_id: 0.2662, G/loss_id_psnt: 0.2634, G/loss_cd: 0.0257\n",
            "Elapsed [0:03:27], Iteration [280/10000], G/loss_id: 0.2570, G/loss_id_psnt: 0.2634, G/loss_cd: 0.0220\n",
            "Elapsed [0:03:34], Iteration [290/10000], G/loss_id: 0.2689, G/loss_id_psnt: 0.2527, G/loss_cd: 0.0217\n",
            "Elapsed [0:03:41], Iteration [300/10000], G/loss_id: 0.2725, G/loss_id_psnt: 0.2533, G/loss_cd: 0.0214\n",
            "Elapsed [0:03:49], Iteration [310/10000], G/loss_id: 0.2533, G/loss_id_psnt: 0.2615, G/loss_cd: 0.0241\n",
            "Elapsed [0:03:55], Iteration [320/10000], G/loss_id: 0.2603, G/loss_id_psnt: 0.2500, G/loss_cd: 0.0214\n",
            "Elapsed [0:04:03], Iteration [330/10000], G/loss_id: 0.2656, G/loss_id_psnt: 0.2567, G/loss_cd: 0.0214\n",
            "Elapsed [0:04:10], Iteration [340/10000], G/loss_id: 0.2556, G/loss_id_psnt: 0.2682, G/loss_cd: 0.0166\n",
            "Elapsed [0:04:17], Iteration [350/10000], G/loss_id: 0.2569, G/loss_id_psnt: 0.2604, G/loss_cd: 0.0185\n",
            "Elapsed [0:04:25], Iteration [360/10000], G/loss_id: 0.2495, G/loss_id_psnt: 0.2621, G/loss_cd: 0.0158\n",
            "Elapsed [0:04:32], Iteration [370/10000], G/loss_id: 0.2506, G/loss_id_psnt: 0.2503, G/loss_cd: 0.0140\n",
            "Elapsed [0:04:40], Iteration [380/10000], G/loss_id: 0.2528, G/loss_id_psnt: 0.2570, G/loss_cd: 0.0134\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "- resolved checkpoints not getting saved \n",
        "- lower checkpoint interval in hparams\n",
        "- add early stopping \n",
        "- Wrote simplified pipeline for inference"
      ],
      "metadata": {
        "id": "S_ilS0muE80n"
      }
    },
    {
      "cell_type": "code",
      "source": [
        ""
      ],
      "metadata": {
        "id": "nF-HazKxJZXd"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        ""
      ],
      "metadata": {
        "id": "jWt1kb7SKe1t"
      }
    }
  ]
}